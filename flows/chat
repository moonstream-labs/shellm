#!/bin/sh
# ~/.config/shellm/flows/chat

# Streaming output via bat and saving to a temporary file
bat_stream() {
	bat --color=always \
		--wrap=character \
		--unbuffered \
		--file-name "LlamaResponse.md" \
		--theme "TwoDark" \
		--tabs=4 \
		--paging=never \
		--pager "less --RAW-CONTROL-CHARS --quit-if-one-screen --mouse" \
		--style "numbers,grid,header-filename"
}

# Interacts with `llm` CLI API, Streams output via `bat_stream_and_save`
chat_with_llm() {
	local input="$1"
	llm <<<"$input" -c | bat_stream
}

# Read initial input
read -r input

# Process initial input
chat_with_llm "$input"

# Main chat loop for continuous interaction with the LLM
while true; do
	read -r command

	if [[ $command == "q" ]]; then
		break
	else
		chat_with_llm "$command"
	fi
done

# Clean up temporary file
rm /tmp/_output
